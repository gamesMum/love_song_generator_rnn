{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Love_Song_Generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1McA9LEO6+bq2Qfft+YO+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gamesMum/love_song_generator_rnn/blob/master/Love_Song_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Ptj_ps7N_D",
        "colab_type": "text"
      },
      "source": [
        "Importing the data from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NIku7Fk6iMh",
        "colab_type": "code",
        "outputId": "a5c552e8-02cd-41c9-c59d-0e08ab1ac121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43xvpcl58awE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing some importent libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFqFcjsY7wwk",
        "colab_type": "code",
        "outputId": "abbe99b6-62d6-4a61-9c78-7d973f76e59c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Let's open the text file and read it\n",
        "with open('/content/drive/My Drive/data/love_songs.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "\n",
        "text[:100]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Doesn't take much to make me happy\\nAnd make me smile with glee \\nNever never will I feel discouraged \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byR8A5bp89pR",
        "colab_type": "text"
      },
      "source": [
        "## **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKnooBRs8zQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Encode the text and map each character to an integer and vice versa\n",
        "chars = tuple(set(text)) #search this\n",
        "int2char = dict(enumerate(chars)) #keys are integer, values are chars\n",
        "char2int = {ch: ii for ii, ch in int2char.items()} #keys are characters, values are values\n",
        "\n",
        "#encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d76ipB2097RF",
        "colab_type": "code",
        "outputId": "5456184a-11e7-47a9-e490-eb69663d6380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoded[:100]\n",
        "print()\n",
        "encoded.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(902550,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gHmiYq1--mg",
        "colab_type": "text"
      },
      "source": [
        "# **Pre-processing the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xP0fxc1-IeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "  #initialize the encoded array with zeros\n",
        "  one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
        "\n",
        "  #fill the approperiate elemnts with ones\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1 #flatten makes 1D array\n",
        "\n",
        "  #reshape it to the oriinal array\n",
        "  one_hot = one_hot.reshape((*arr.shape, n_labels)) #why th *\n",
        "\n",
        "  return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VtN-gg7AaYr",
        "colab_type": "code",
        "outputId": "838e9124-0267-4f15-e40b-200f5250080a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Test\n",
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkK_hJ8VAm5q",
        "colab_type": "text"
      },
      "source": [
        "## **Making the min-batching**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGQg3SZPAhJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "  #number of characters in a complete min batch\n",
        "  batch_size_total = batch_size * seq_length\n",
        "\n",
        "  #Get the number of batches\n",
        "  n_batches = len(arr) // batch_size_total\n",
        "\n",
        "  #keep only enough characters to make full batches\n",
        "  arr = arr[: n_batches * batch_size_total]\n",
        "  #reshape into batch_size row\n",
        "  arr = arr.reshape(batch_size, -1)\n",
        "  #Iterate through the batches using window of size seq_length\n",
        "  for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features (rows are the batches, columns are the seq_length window)\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "          y[:, :-1], y[:, -1] = x[: , 1:], arr[:, n+seq_length] #y = x hifted by one\n",
        "        except IndexError:\n",
        "          y[:, :-1], y[:, -1] = x[: , 1:], arr[:, 0]\n",
        "        yield x, y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBuzL1UxMxBn",
        "colab_type": "text"
      },
      "source": [
        "## **Define the network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnxS4FPrFpaq",
        "colab_type": "code",
        "outputId": "659c0d05-cd24-49f0-b2f6-fe09695ed31c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " #check if gpu is available\n",
        " train_on_gpu = torch.cuda.is_available()\n",
        " if(train_on_gpu):\n",
        "   print('Training on GPU')\n",
        " else:\n",
        "   print('No GPU is available. Trainig on CPU')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP8Tf0ZV1KOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define our Networ\n",
        "class CharRNN(nn.Module):\n",
        "  def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "               drop_prob=0.25, lr=0.01):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "\n",
        "    #create charachters dictionary\n",
        "    self.chars =  tokens\n",
        "    self.int2char = dict(enumerate(self.chars))\n",
        "    self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "\n",
        "    #Define the layers of the model\n",
        "    self.lstm = nn.LSTM(len(chars), n_hidden, n_layers,\n",
        "                        dropout=drop_prob, batch_first=True)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    # x (batch_size, seq_length, input_size)\n",
        "    # hidden (n_layers, batch_size, hidden_dim)\n",
        "    # r_out (batch_size, time_step, hidden_size)   \n",
        "    ## TODO: Get the outputs and the new hidden state from the lstm\n",
        "\n",
        "    #get LSTM outputs\n",
        "    r_out, hidden = self.lstm(x, hidden)\n",
        "    #pass it through the dropout layer\n",
        "    out = self.dropout(r_out)\n",
        "\n",
        "    #stack up LSTM outputs using view \n",
        "    #use contiguous to reashape the output\n",
        "    out = out.contiguous().view(-1, self.n_hidden)\n",
        "    #now pass it through the fully connected layer\n",
        "    out = self.fc(out)\n",
        "\n",
        "    #return the final output and the hidden state\n",
        "    return out, hidden\n",
        "  def init_hidden(self, batch_size):\n",
        "    # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "    # initialized to zero, for hidden state and cell state of LSTM\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    if (train_on_gpu):\n",
        "       hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "    return hidden\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shefeQSTDsGf",
        "colab_type": "text"
      },
      "source": [
        "## **Train our model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLylwKWNCy2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h) #h: contains all information from all the previous steps\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvEg0OXnD0ZC",
        "colab_type": "code",
        "outputId": "0446cfdb-5db8-4db4-97db-61cca7d8354e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# define and print the net\n",
        "n_hidden= 128\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers, drop_prob=0.25)\n",
        "print(net)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(99, 128, num_layers=2, batch_first=True, dropout=0.25)\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            "  (fc): Linear(in_features=128, out_features=99, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHxRDnwjD8BA",
        "colab_type": "code",
        "outputId": "d54a881d-d9cf-4634-8603-929165b265ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 100 \n",
        "seq_length = 64\n",
        "n_epochs =  50 # start small if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.002, print_every=10)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50... Step: 10... Loss: 3.2930... Val Loss: 3.2831\n",
            "Epoch: 1/50... Step: 20... Loss: 3.1826... Val Loss: 3.2226\n",
            "Epoch: 1/50... Step: 30... Loss: 3.1601... Val Loss: 3.2035\n",
            "Epoch: 1/50... Step: 40... Loss: 3.1499... Val Loss: 3.1960\n",
            "Epoch: 1/50... Step: 50... Loss: 3.1475... Val Loss: 3.1958\n",
            "Epoch: 1/50... Step: 60... Loss: 3.1381... Val Loss: 3.1930\n",
            "Epoch: 1/50... Step: 70... Loss: 3.1485... Val Loss: 3.1934\n",
            "Epoch: 1/50... Step: 80... Loss: 3.1378... Val Loss: 3.1924\n",
            "Epoch: 1/50... Step: 90... Loss: 3.1709... Val Loss: 3.1922\n",
            "Epoch: 1/50... Step: 100... Loss: 3.1276... Val Loss: 3.1898\n",
            "Epoch: 1/50... Step: 110... Loss: 3.1533... Val Loss: 3.1868\n",
            "Epoch: 1/50... Step: 120... Loss: 3.1318... Val Loss: 3.1787\n",
            "Epoch: 2/50... Step: 130... Loss: 3.1249... Val Loss: 3.1647\n",
            "Epoch: 2/50... Step: 140... Loss: 3.0818... Val Loss: 3.1340\n",
            "Epoch: 2/50... Step: 150... Loss: 3.0097... Val Loss: 3.0830\n",
            "Epoch: 2/50... Step: 160... Loss: 2.9631... Val Loss: 3.0176\n",
            "Epoch: 2/50... Step: 170... Loss: 2.9212... Val Loss: 2.9597\n",
            "Epoch: 2/50... Step: 180... Loss: 2.8417... Val Loss: 2.9121\n",
            "Epoch: 2/50... Step: 190... Loss: 2.8109... Val Loss: 2.8649\n",
            "Epoch: 2/50... Step: 200... Loss: 2.7739... Val Loss: 2.8191\n",
            "Epoch: 2/50... Step: 210... Loss: 2.7348... Val Loss: 2.7690\n",
            "Epoch: 2/50... Step: 220... Loss: 2.6606... Val Loss: 2.7301\n",
            "Epoch: 2/50... Step: 230... Loss: 2.6214... Val Loss: 2.6897\n",
            "Epoch: 2/50... Step: 240... Loss: 2.5746... Val Loss: 2.6545\n",
            "Epoch: 2/50... Step: 250... Loss: 2.5740... Val Loss: 2.6221\n",
            "Epoch: 3/50... Step: 260... Loss: 2.5233... Val Loss: 2.5947\n",
            "Epoch: 3/50... Step: 270... Loss: 2.5278... Val Loss: 2.5661\n",
            "Epoch: 3/50... Step: 280... Loss: 2.4863... Val Loss: 2.5411\n",
            "Epoch: 3/50... Step: 290... Loss: 2.4372... Val Loss: 2.5174\n",
            "Epoch: 3/50... Step: 300... Loss: 2.4273... Val Loss: 2.4935\n",
            "Epoch: 3/50... Step: 310... Loss: 2.4361... Val Loss: 2.4796\n",
            "Epoch: 3/50... Step: 320... Loss: 2.3908... Val Loss: 2.4558\n",
            "Epoch: 3/50... Step: 330... Loss: 2.3220... Val Loss: 2.4439\n",
            "Epoch: 3/50... Step: 340... Loss: 2.3229... Val Loss: 2.4215\n",
            "Epoch: 3/50... Step: 350... Loss: 2.3678... Val Loss: 2.4010\n",
            "Epoch: 3/50... Step: 360... Loss: 2.3225... Val Loss: 2.3844\n",
            "Epoch: 3/50... Step: 370... Loss: 2.3134... Val Loss: 2.3729\n",
            "Epoch: 4/50... Step: 380... Loss: 2.2933... Val Loss: 2.3601\n",
            "Epoch: 4/50... Step: 390... Loss: 2.2342... Val Loss: 2.3508\n",
            "Epoch: 4/50... Step: 400... Loss: 2.2554... Val Loss: 2.3336\n",
            "Epoch: 4/50... Step: 410... Loss: 2.2507... Val Loss: 2.3270\n",
            "Epoch: 4/50... Step: 420... Loss: 2.2173... Val Loss: 2.3118\n",
            "Epoch: 4/50... Step: 430... Loss: 2.2193... Val Loss: 2.2993\n",
            "Epoch: 4/50... Step: 440... Loss: 2.2005... Val Loss: 2.2882\n",
            "Epoch: 4/50... Step: 450... Loss: 2.2335... Val Loss: 2.2752\n",
            "Epoch: 4/50... Step: 460... Loss: 2.1360... Val Loss: 2.2687\n",
            "Epoch: 4/50... Step: 470... Loss: 2.1388... Val Loss: 2.2546\n",
            "Epoch: 4/50... Step: 480... Loss: 2.1522... Val Loss: 2.2421\n",
            "Epoch: 4/50... Step: 490... Loss: 2.1507... Val Loss: 2.2342\n",
            "Epoch: 4/50... Step: 500... Loss: 2.1606... Val Loss: 2.2246\n",
            "Epoch: 5/50... Step: 510... Loss: 2.0858... Val Loss: 2.2150\n",
            "Epoch: 5/50... Step: 520... Loss: 2.1084... Val Loss: 2.2049\n",
            "Epoch: 5/50... Step: 530... Loss: 2.0755... Val Loss: 2.1999\n",
            "Epoch: 5/50... Step: 540... Loss: 2.0745... Val Loss: 2.1881\n",
            "Epoch: 5/50... Step: 550... Loss: 2.0940... Val Loss: 2.1794\n",
            "Epoch: 5/50... Step: 560... Loss: 2.0143... Val Loss: 2.1703\n",
            "Epoch: 5/50... Step: 570... Loss: 2.0700... Val Loss: 2.1628\n",
            "Epoch: 5/50... Step: 580... Loss: 2.0179... Val Loss: 2.1542\n",
            "Epoch: 5/50... Step: 590... Loss: 2.0418... Val Loss: 2.1462\n",
            "Epoch: 5/50... Step: 600... Loss: 2.0448... Val Loss: 2.1351\n",
            "Epoch: 5/50... Step: 610... Loss: 2.0364... Val Loss: 2.1291\n",
            "Epoch: 5/50... Step: 620... Loss: 1.9913... Val Loss: 2.1222\n",
            "Epoch: 5/50... Step: 630... Loss: 2.0398... Val Loss: 2.1204\n",
            "Epoch: 6/50... Step: 640... Loss: 2.0364... Val Loss: 2.1104\n",
            "Epoch: 6/50... Step: 650... Loss: 2.0127... Val Loss: 2.1044\n",
            "Epoch: 6/50... Step: 660... Loss: 1.9911... Val Loss: 2.1017\n",
            "Epoch: 6/50... Step: 670... Loss: 1.9903... Val Loss: 2.0933\n",
            "Epoch: 6/50... Step: 680... Loss: 1.9380... Val Loss: 2.0854\n",
            "Epoch: 6/50... Step: 690... Loss: 1.9504... Val Loss: 2.0812\n",
            "Epoch: 6/50... Step: 700... Loss: 1.9733... Val Loss: 2.0749\n",
            "Epoch: 6/50... Step: 710... Loss: 1.9692... Val Loss: 2.0676\n",
            "Epoch: 6/50... Step: 720... Loss: 1.9714... Val Loss: 2.0616\n",
            "Epoch: 6/50... Step: 730... Loss: 1.8944... Val Loss: 2.0544\n",
            "Epoch: 6/50... Step: 740... Loss: 1.9274... Val Loss: 2.0495\n",
            "Epoch: 6/50... Step: 750... Loss: 1.9278... Val Loss: 2.0442\n",
            "Epoch: 7/50... Step: 760... Loss: 1.9800... Val Loss: 2.0414\n",
            "Epoch: 7/50... Step: 770... Loss: 1.9087... Val Loss: 2.0336\n",
            "Epoch: 7/50... Step: 780... Loss: 1.8872... Val Loss: 2.0333\n",
            "Epoch: 7/50... Step: 790... Loss: 1.9005... Val Loss: 2.0287\n",
            "Epoch: 7/50... Step: 800... Loss: 1.9115... Val Loss: 2.0243\n",
            "Epoch: 7/50... Step: 810... Loss: 1.8890... Val Loss: 2.0181\n",
            "Epoch: 7/50... Step: 820... Loss: 1.9260... Val Loss: 2.0120\n",
            "Epoch: 7/50... Step: 830... Loss: 1.8748... Val Loss: 2.0073\n",
            "Epoch: 7/50... Step: 840... Loss: 1.8520... Val Loss: 2.0033\n",
            "Epoch: 7/50... Step: 850... Loss: 1.8699... Val Loss: 2.0005\n",
            "Epoch: 7/50... Step: 860... Loss: 1.8636... Val Loss: 1.9941\n",
            "Epoch: 7/50... Step: 870... Loss: 1.8362... Val Loss: 1.9948\n",
            "Epoch: 7/50... Step: 880... Loss: 1.9120... Val Loss: 1.9868\n",
            "Epoch: 8/50... Step: 890... Loss: 1.8902... Val Loss: 1.9823\n",
            "Epoch: 8/50... Step: 900... Loss: 1.8547... Val Loss: 1.9779\n",
            "Epoch: 8/50... Step: 910... Loss: 1.8348... Val Loss: 1.9760\n",
            "Epoch: 8/50... Step: 920... Loss: 1.8037... Val Loss: 1.9730\n",
            "Epoch: 8/50... Step: 930... Loss: 1.8344... Val Loss: 1.9713\n",
            "Epoch: 8/50... Step: 940... Loss: 1.8738... Val Loss: 1.9701\n",
            "Epoch: 8/50... Step: 950... Loss: 1.8486... Val Loss: 1.9582\n",
            "Epoch: 8/50... Step: 960... Loss: 1.8021... Val Loss: 1.9594\n",
            "Epoch: 8/50... Step: 970... Loss: 1.8097... Val Loss: 1.9534\n",
            "Epoch: 8/50... Step: 980... Loss: 1.8430... Val Loss: 1.9466\n",
            "Epoch: 8/50... Step: 990... Loss: 1.8473... Val Loss: 1.9488\n",
            "Epoch: 8/50... Step: 1000... Loss: 1.8516... Val Loss: 1.9405\n",
            "Epoch: 9/50... Step: 1010... Loss: 1.8303... Val Loss: 1.9379\n",
            "Epoch: 9/50... Step: 1020... Loss: 1.7843... Val Loss: 1.9318\n",
            "Epoch: 9/50... Step: 1030... Loss: 1.7715... Val Loss: 1.9320\n",
            "Epoch: 9/50... Step: 1040... Loss: 1.7889... Val Loss: 1.9337\n",
            "Epoch: 9/50... Step: 1050... Loss: 1.7897... Val Loss: 1.9300\n",
            "Epoch: 9/50... Step: 1060... Loss: 1.7966... Val Loss: 1.9230\n",
            "Epoch: 9/50... Step: 1070... Loss: 1.7844... Val Loss: 1.9187\n",
            "Epoch: 9/50... Step: 1080... Loss: 1.8065... Val Loss: 1.9202\n",
            "Epoch: 9/50... Step: 1090... Loss: 1.7498... Val Loss: 1.9133\n",
            "Epoch: 9/50... Step: 1100... Loss: 1.7621... Val Loss: 1.9108\n",
            "Epoch: 9/50... Step: 1110... Loss: 1.7770... Val Loss: 1.9088\n",
            "Epoch: 9/50... Step: 1120... Loss: 1.7877... Val Loss: 1.9021\n",
            "Epoch: 9/50... Step: 1130... Loss: 1.7969... Val Loss: 1.9001\n",
            "Epoch: 10/50... Step: 1140... Loss: 1.7292... Val Loss: 1.9002\n",
            "Epoch: 10/50... Step: 1150... Loss: 1.7494... Val Loss: 1.8950\n",
            "Epoch: 10/50... Step: 1160... Loss: 1.7520... Val Loss: 1.8946\n",
            "Epoch: 10/50... Step: 1170... Loss: 1.7434... Val Loss: 1.8961\n",
            "Epoch: 10/50... Step: 1180... Loss: 1.7918... Val Loss: 1.8895\n",
            "Epoch: 10/50... Step: 1190... Loss: 1.7163... Val Loss: 1.8908\n",
            "Epoch: 10/50... Step: 1200... Loss: 1.7870... Val Loss: 1.8834\n",
            "Epoch: 10/50... Step: 1210... Loss: 1.7095... Val Loss: 1.8825\n",
            "Epoch: 10/50... Step: 1220... Loss: 1.7488... Val Loss: 1.8813\n",
            "Epoch: 10/50... Step: 1230... Loss: 1.7336... Val Loss: 1.8760\n",
            "Epoch: 10/50... Step: 1240... Loss: 1.7425... Val Loss: 1.8724\n",
            "Epoch: 10/50... Step: 1250... Loss: 1.7134... Val Loss: 1.8713\n",
            "Epoch: 10/50... Step: 1260... Loss: 1.7663... Val Loss: 1.8670\n",
            "Epoch: 11/50... Step: 1270... Loss: 1.7617... Val Loss: 1.8684\n",
            "Epoch: 11/50... Step: 1280... Loss: 1.7347... Val Loss: 1.8662\n",
            "Epoch: 11/50... Step: 1290... Loss: 1.7204... Val Loss: 1.8649\n",
            "Epoch: 11/50... Step: 1300... Loss: 1.7335... Val Loss: 1.8602\n",
            "Epoch: 11/50... Step: 1310... Loss: 1.6938... Val Loss: 1.8608\n",
            "Epoch: 11/50... Step: 1320... Loss: 1.7063... Val Loss: 1.8568\n",
            "Epoch: 11/50... Step: 1330... Loss: 1.7297... Val Loss: 1.8520\n",
            "Epoch: 11/50... Step: 1340... Loss: 1.7093... Val Loss: 1.8547\n",
            "Epoch: 11/50... Step: 1350... Loss: 1.7139... Val Loss: 1.8494\n",
            "Epoch: 11/50... Step: 1360... Loss: 1.6614... Val Loss: 1.8461\n",
            "Epoch: 11/50... Step: 1370... Loss: 1.6878... Val Loss: 1.8487\n",
            "Epoch: 11/50... Step: 1380... Loss: 1.6967... Val Loss: 1.8410\n",
            "Epoch: 12/50... Step: 1390... Loss: 1.7605... Val Loss: 1.8392\n",
            "Epoch: 12/50... Step: 1400... Loss: 1.6775... Val Loss: 1.8348\n",
            "Epoch: 12/50... Step: 1410... Loss: 1.6707... Val Loss: 1.8421\n",
            "Epoch: 12/50... Step: 1420... Loss: 1.6835... Val Loss: 1.8383\n",
            "Epoch: 12/50... Step: 1430... Loss: 1.7024... Val Loss: 1.8360\n",
            "Epoch: 12/50... Step: 1440... Loss: 1.6821... Val Loss: 1.8329\n",
            "Epoch: 12/50... Step: 1450... Loss: 1.7137... Val Loss: 1.8292\n",
            "Epoch: 12/50... Step: 1460... Loss: 1.6691... Val Loss: 1.8289\n",
            "Epoch: 12/50... Step: 1470... Loss: 1.6374... Val Loss: 1.8249\n",
            "Epoch: 12/50... Step: 1480... Loss: 1.6629... Val Loss: 1.8284\n",
            "Epoch: 12/50... Step: 1490... Loss: 1.6700... Val Loss: 1.8228\n",
            "Epoch: 12/50... Step: 1500... Loss: 1.6502... Val Loss: 1.8174\n",
            "Epoch: 12/50... Step: 1510... Loss: 1.7189... Val Loss: 1.8176\n",
            "Epoch: 13/50... Step: 1520... Loss: 1.6989... Val Loss: 1.8145\n",
            "Epoch: 13/50... Step: 1530... Loss: 1.6639... Val Loss: 1.8148\n",
            "Epoch: 13/50... Step: 1540... Loss: 1.6658... Val Loss: 1.8160\n",
            "Epoch: 13/50... Step: 1550... Loss: 1.6260... Val Loss: 1.8122\n",
            "Epoch: 13/50... Step: 1560... Loss: 1.6501... Val Loss: 1.8103\n",
            "Epoch: 13/50... Step: 1570... Loss: 1.7058... Val Loss: 1.8078\n",
            "Epoch: 13/50... Step: 1580... Loss: 1.6718... Val Loss: 1.8037\n",
            "Epoch: 13/50... Step: 1590... Loss: 1.6440... Val Loss: 1.8033\n",
            "Epoch: 13/50... Step: 1600... Loss: 1.6395... Val Loss: 1.8031\n",
            "Epoch: 13/50... Step: 1610... Loss: 1.6801... Val Loss: 1.7978\n",
            "Epoch: 13/50... Step: 1620... Loss: 1.6644... Val Loss: 1.7955\n",
            "Epoch: 13/50... Step: 1630... Loss: 1.6877... Val Loss: 1.7924\n",
            "Epoch: 14/50... Step: 1640... Loss: 1.6576... Val Loss: 1.7938\n",
            "Epoch: 14/50... Step: 1650... Loss: 1.6226... Val Loss: 1.7913\n",
            "Epoch: 14/50... Step: 1660... Loss: 1.6049... Val Loss: 1.7918\n",
            "Epoch: 14/50... Step: 1670... Loss: 1.6371... Val Loss: 1.7893\n",
            "Epoch: 14/50... Step: 1680... Loss: 1.6389... Val Loss: 1.7907\n",
            "Epoch: 14/50... Step: 1690... Loss: 1.6514... Val Loss: 1.7870\n",
            "Epoch: 14/50... Step: 1700... Loss: 1.6116... Val Loss: 1.7866\n",
            "Epoch: 14/50... Step: 1710... Loss: 1.6476... Val Loss: 1.7809\n",
            "Epoch: 14/50... Step: 1720... Loss: 1.5992... Val Loss: 1.7828\n",
            "Epoch: 14/50... Step: 1730... Loss: 1.6176... Val Loss: 1.7806\n",
            "Epoch: 14/50... Step: 1740... Loss: 1.6142... Val Loss: 1.7811\n",
            "Epoch: 14/50... Step: 1750... Loss: 1.6632... Val Loss: 1.7731\n",
            "Epoch: 14/50... Step: 1760... Loss: 1.6453... Val Loss: 1.7732\n",
            "Epoch: 15/50... Step: 1770... Loss: 1.5953... Val Loss: 1.7687\n",
            "Epoch: 15/50... Step: 1780... Loss: 1.6124... Val Loss: 1.7712\n",
            "Epoch: 15/50... Step: 1790... Loss: 1.6189... Val Loss: 1.7727\n",
            "Epoch: 15/50... Step: 1800... Loss: 1.6130... Val Loss: 1.7733\n",
            "Epoch: 15/50... Step: 1810... Loss: 1.6589... Val Loss: 1.7676\n",
            "Epoch: 15/50... Step: 1820... Loss: 1.5875... Val Loss: 1.7717\n",
            "Epoch: 15/50... Step: 1830... Loss: 1.6475... Val Loss: 1.7633\n",
            "Epoch: 15/50... Step: 1840... Loss: 1.5772... Val Loss: 1.7628\n",
            "Epoch: 15/50... Step: 1850... Loss: 1.6105... Val Loss: 1.7628\n",
            "Epoch: 15/50... Step: 1860... Loss: 1.6026... Val Loss: 1.7657\n",
            "Epoch: 15/50... Step: 1870... Loss: 1.6312... Val Loss: 1.7579\n",
            "Epoch: 15/50... Step: 1880... Loss: 1.5909... Val Loss: 1.7572\n",
            "Epoch: 15/50... Step: 1890... Loss: 1.6538... Val Loss: 1.7534\n",
            "Epoch: 16/50... Step: 1900... Loss: 1.6205... Val Loss: 1.7541\n",
            "Epoch: 16/50... Step: 1910... Loss: 1.6030... Val Loss: 1.7548\n",
            "Epoch: 16/50... Step: 1920... Loss: 1.6023... Val Loss: 1.7541\n",
            "Epoch: 16/50... Step: 1930... Loss: 1.6027... Val Loss: 1.7514\n",
            "Epoch: 16/50... Step: 1940... Loss: 1.5828... Val Loss: 1.7513\n",
            "Epoch: 16/50... Step: 1950... Loss: 1.5804... Val Loss: 1.7498\n",
            "Epoch: 16/50... Step: 1960... Loss: 1.6113... Val Loss: 1.7449\n",
            "Epoch: 16/50... Step: 1970... Loss: 1.5807... Val Loss: 1.7434\n",
            "Epoch: 16/50... Step: 1980... Loss: 1.5803... Val Loss: 1.7462\n",
            "Epoch: 16/50... Step: 1990... Loss: 1.5602... Val Loss: 1.7391\n",
            "Epoch: 16/50... Step: 2000... Loss: 1.5887... Val Loss: 1.7433\n",
            "Epoch: 16/50... Step: 2010... Loss: 1.5803... Val Loss: 1.7363\n",
            "Epoch: 17/50... Step: 2020... Loss: 1.6431... Val Loss: 1.7368\n",
            "Epoch: 17/50... Step: 2030... Loss: 1.5531... Val Loss: 1.7347\n",
            "Epoch: 17/50... Step: 2040... Loss: 1.5478... Val Loss: 1.7379\n",
            "Epoch: 17/50... Step: 2050... Loss: 1.5839... Val Loss: 1.7361\n",
            "Epoch: 17/50... Step: 2060... Loss: 1.5921... Val Loss: 1.7408\n",
            "Epoch: 17/50... Step: 2070... Loss: 1.5611... Val Loss: 1.7325\n",
            "Epoch: 17/50... Step: 2080... Loss: 1.5883... Val Loss: 1.7342\n",
            "Epoch: 17/50... Step: 2090... Loss: 1.5614... Val Loss: 1.7295\n",
            "Epoch: 17/50... Step: 2100... Loss: 1.5334... Val Loss: 1.7303\n",
            "Epoch: 17/50... Step: 2110... Loss: 1.5424... Val Loss: 1.7287\n",
            "Epoch: 17/50... Step: 2120... Loss: 1.5618... Val Loss: 1.7280\n",
            "Epoch: 17/50... Step: 2130... Loss: 1.5504... Val Loss: 1.7222\n",
            "Epoch: 17/50... Step: 2140... Loss: 1.5991... Val Loss: 1.7237\n",
            "Epoch: 18/50... Step: 2150... Loss: 1.5803... Val Loss: 1.7241\n",
            "Epoch: 18/50... Step: 2160... Loss: 1.5431... Val Loss: 1.7251\n",
            "Epoch: 18/50... Step: 2170... Loss: 1.5579... Val Loss: 1.7270\n",
            "Epoch: 18/50... Step: 2180... Loss: 1.5158... Val Loss: 1.7256\n",
            "Epoch: 18/50... Step: 2190... Loss: 1.5315... Val Loss: 1.7203\n",
            "Epoch: 18/50... Step: 2200... Loss: 1.5938... Val Loss: 1.7197\n",
            "Epoch: 18/50... Step: 2210... Loss: 1.5764... Val Loss: 1.7131\n",
            "Epoch: 18/50... Step: 2220... Loss: 1.5352... Val Loss: 1.7131\n",
            "Epoch: 18/50... Step: 2230... Loss: 1.5406... Val Loss: 1.7148\n",
            "Epoch: 18/50... Step: 2240... Loss: 1.5717... Val Loss: 1.7112\n",
            "Epoch: 18/50... Step: 2250... Loss: 1.5714... Val Loss: 1.7106\n",
            "Epoch: 18/50... Step: 2260... Loss: 1.5989... Val Loss: 1.7095\n",
            "Epoch: 19/50... Step: 2270... Loss: 1.5544... Val Loss: 1.7069\n",
            "Epoch: 19/50... Step: 2280... Loss: 1.5269... Val Loss: 1.7081\n",
            "Epoch: 19/50... Step: 2290... Loss: 1.5018... Val Loss: 1.7105\n",
            "Epoch: 19/50... Step: 2300... Loss: 1.5337... Val Loss: 1.7077\n",
            "Epoch: 19/50... Step: 2310... Loss: 1.5517... Val Loss: 1.7069\n",
            "Epoch: 19/50... Step: 2320... Loss: 1.5501... Val Loss: 1.7039\n",
            "Epoch: 19/50... Step: 2330... Loss: 1.5339... Val Loss: 1.7047\n",
            "Epoch: 19/50... Step: 2340... Loss: 1.5619... Val Loss: 1.7006\n",
            "Epoch: 19/50... Step: 2350... Loss: 1.5024... Val Loss: 1.7006\n",
            "Epoch: 19/50... Step: 2360... Loss: 1.5214... Val Loss: 1.7003\n",
            "Epoch: 19/50... Step: 2370... Loss: 1.5119... Val Loss: 1.6975\n",
            "Epoch: 19/50... Step: 2380... Loss: 1.5622... Val Loss: 1.6932\n",
            "Epoch: 19/50... Step: 2390... Loss: 1.5500... Val Loss: 1.6918\n",
            "Epoch: 20/50... Step: 2400... Loss: 1.4986... Val Loss: 1.6934\n",
            "Epoch: 20/50... Step: 2410... Loss: 1.5119... Val Loss: 1.6926\n",
            "Epoch: 20/50... Step: 2420... Loss: 1.5184... Val Loss: 1.6951\n",
            "Epoch: 20/50... Step: 2430... Loss: 1.5113... Val Loss: 1.6953\n",
            "Epoch: 20/50... Step: 2440... Loss: 1.5659... Val Loss: 1.6926\n",
            "Epoch: 20/50... Step: 2450... Loss: 1.4785... Val Loss: 1.6936\n",
            "Epoch: 20/50... Step: 2460... Loss: 1.5401... Val Loss: 1.6876\n",
            "Epoch: 20/50... Step: 2470... Loss: 1.4845... Val Loss: 1.6887\n",
            "Epoch: 20/50... Step: 2480... Loss: 1.5158... Val Loss: 1.6849\n",
            "Epoch: 20/50... Step: 2490... Loss: 1.5053... Val Loss: 1.6832\n",
            "Epoch: 20/50... Step: 2500... Loss: 1.5322... Val Loss: 1.6824\n",
            "Epoch: 20/50... Step: 2510... Loss: 1.4917... Val Loss: 1.6801\n",
            "Epoch: 20/50... Step: 2520... Loss: 1.5642... Val Loss: 1.6792\n",
            "Epoch: 21/50... Step: 2530... Loss: 1.5139... Val Loss: 1.6793\n",
            "Epoch: 21/50... Step: 2540... Loss: 1.5076... Val Loss: 1.6816\n",
            "Epoch: 21/50... Step: 2550... Loss: 1.5110... Val Loss: 1.6849\n",
            "Epoch: 21/50... Step: 2560... Loss: 1.5153... Val Loss: 1.6774\n",
            "Epoch: 21/50... Step: 2570... Loss: 1.4948... Val Loss: 1.6784\n",
            "Epoch: 21/50... Step: 2580... Loss: 1.4969... Val Loss: 1.6765\n",
            "Epoch: 21/50... Step: 2590... Loss: 1.5261... Val Loss: 1.6719\n",
            "Epoch: 21/50... Step: 2600... Loss: 1.4806... Val Loss: 1.6714\n",
            "Epoch: 21/50... Step: 2610... Loss: 1.5002... Val Loss: 1.6735\n",
            "Epoch: 21/50... Step: 2620... Loss: 1.4797... Val Loss: 1.6683\n",
            "Epoch: 21/50... Step: 2630... Loss: 1.4927... Val Loss: 1.6688\n",
            "Epoch: 21/50... Step: 2640... Loss: 1.4985... Val Loss: 1.6643\n",
            "Epoch: 22/50... Step: 2650... Loss: 1.5448... Val Loss: 1.6628\n",
            "Epoch: 22/50... Step: 2660... Loss: 1.4765... Val Loss: 1.6689\n",
            "Epoch: 22/50... Step: 2670... Loss: 1.4626... Val Loss: 1.6665\n",
            "Epoch: 22/50... Step: 2680... Loss: 1.4906... Val Loss: 1.6641\n",
            "Epoch: 22/50... Step: 2690... Loss: 1.5087... Val Loss: 1.6680\n",
            "Epoch: 22/50... Step: 2700... Loss: 1.4874... Val Loss: 1.6634\n",
            "Epoch: 22/50... Step: 2710... Loss: 1.5047... Val Loss: 1.6634\n",
            "Epoch: 22/50... Step: 2720... Loss: 1.4777... Val Loss: 1.6586\n",
            "Epoch: 22/50... Step: 2730... Loss: 1.4450... Val Loss: 1.6608\n",
            "Epoch: 22/50... Step: 2740... Loss: 1.4533... Val Loss: 1.6568\n",
            "Epoch: 22/50... Step: 2750... Loss: 1.4711... Val Loss: 1.6597\n",
            "Epoch: 22/50... Step: 2760... Loss: 1.4495... Val Loss: 1.6526\n",
            "Epoch: 22/50... Step: 2770... Loss: 1.5053... Val Loss: 1.6526\n",
            "Epoch: 23/50... Step: 2780... Loss: 1.4952... Val Loss: 1.6557\n",
            "Epoch: 23/50... Step: 2790... Loss: 1.4676... Val Loss: 1.6565\n",
            "Epoch: 23/50... Step: 2800... Loss: 1.4755... Val Loss: 1.6588\n",
            "Epoch: 23/50... Step: 2810... Loss: 1.4322... Val Loss: 1.6624\n",
            "Epoch: 23/50... Step: 2820... Loss: 1.4591... Val Loss: 1.6521\n",
            "Epoch: 23/50... Step: 2830... Loss: 1.4938... Val Loss: 1.6506\n",
            "Epoch: 23/50... Step: 2840... Loss: 1.4909... Val Loss: 1.6501\n",
            "Epoch: 23/50... Step: 2850... Loss: 1.4665... Val Loss: 1.6486\n",
            "Epoch: 23/50... Step: 2860... Loss: 1.4486... Val Loss: 1.6494\n",
            "Epoch: 23/50... Step: 2870... Loss: 1.4761... Val Loss: 1.6426\n",
            "Epoch: 23/50... Step: 2880... Loss: 1.4791... Val Loss: 1.6425\n",
            "Epoch: 23/50... Step: 2890... Loss: 1.5072... Val Loss: 1.6457\n",
            "Epoch: 24/50... Step: 2900... Loss: 1.4658... Val Loss: 1.6394\n",
            "Epoch: 24/50... Step: 2910... Loss: 1.4480... Val Loss: 1.6380\n",
            "Epoch: 24/50... Step: 2920... Loss: 1.4292... Val Loss: 1.6489\n",
            "Epoch: 24/50... Step: 2930... Loss: 1.4628... Val Loss: 1.6449\n",
            "Epoch: 24/50... Step: 2940... Loss: 1.4815... Val Loss: 1.6387\n",
            "Epoch: 24/50... Step: 2950... Loss: 1.4718... Val Loss: 1.6393\n",
            "Epoch: 24/50... Step: 2960... Loss: 1.4662... Val Loss: 1.6405\n",
            "Epoch: 24/50... Step: 2970... Loss: 1.4752... Val Loss: 1.6341\n",
            "Epoch: 24/50... Step: 2980... Loss: 1.4259... Val Loss: 1.6363\n",
            "Epoch: 24/50... Step: 2990... Loss: 1.4520... Val Loss: 1.6368\n",
            "Epoch: 24/50... Step: 3000... Loss: 1.4371... Val Loss: 1.6308\n",
            "Epoch: 24/50... Step: 3010... Loss: 1.4952... Val Loss: 1.6292\n",
            "Epoch: 24/50... Step: 3020... Loss: 1.4635... Val Loss: 1.6282\n",
            "Epoch: 25/50... Step: 3030... Loss: 1.4216... Val Loss: 1.6286\n",
            "Epoch: 25/50... Step: 3040... Loss: 1.4353... Val Loss: 1.6364\n",
            "Epoch: 25/50... Step: 3050... Loss: 1.4422... Val Loss: 1.6344\n",
            "Epoch: 25/50... Step: 3060... Loss: 1.4445... Val Loss: 1.6323\n",
            "Epoch: 25/50... Step: 3070... Loss: 1.4732... Val Loss: 1.6360\n",
            "Epoch: 25/50... Step: 3080... Loss: 1.4135... Val Loss: 1.6332\n",
            "Epoch: 25/50... Step: 3090... Loss: 1.4646... Val Loss: 1.6259\n",
            "Epoch: 25/50... Step: 3100... Loss: 1.4175... Val Loss: 1.6294\n",
            "Epoch: 25/50... Step: 3110... Loss: 1.4554... Val Loss: 1.6291\n",
            "Epoch: 25/50... Step: 3120... Loss: 1.4229... Val Loss: 1.6262\n",
            "Epoch: 25/50... Step: 3130... Loss: 1.4517... Val Loss: 1.6219\n",
            "Epoch: 25/50... Step: 3140... Loss: 1.4333... Val Loss: 1.6220\n",
            "Epoch: 25/50... Step: 3150... Loss: 1.5026... Val Loss: 1.6228\n",
            "Epoch: 26/50... Step: 3160... Loss: 1.4475... Val Loss: 1.6205\n",
            "Epoch: 26/50... Step: 3170... Loss: 1.4214... Val Loss: 1.6239\n",
            "Epoch: 26/50... Step: 3180... Loss: 1.4501... Val Loss: 1.6276\n",
            "Epoch: 26/50... Step: 3190... Loss: 1.4656... Val Loss: 1.6207\n",
            "Epoch: 26/50... Step: 3200... Loss: 1.4254... Val Loss: 1.6183\n",
            "Epoch: 26/50... Step: 3210... Loss: 1.4237... Val Loss: 1.6194\n",
            "Epoch: 26/50... Step: 3220... Loss: 1.4489... Val Loss: 1.6196\n",
            "Epoch: 26/50... Step: 3230... Loss: 1.4266... Val Loss: 1.6179\n",
            "Epoch: 26/50... Step: 3240... Loss: 1.4197... Val Loss: 1.6142\n",
            "Epoch: 26/50... Step: 3250... Loss: 1.4113... Val Loss: 1.6132\n",
            "Epoch: 26/50... Step: 3260... Loss: 1.4229... Val Loss: 1.6155\n",
            "Epoch: 26/50... Step: 3270... Loss: 1.4100... Val Loss: 1.6119\n",
            "Epoch: 27/50... Step: 3280... Loss: 1.4739... Val Loss: 1.6129\n",
            "Epoch: 27/50... Step: 3290... Loss: 1.4075... Val Loss: 1.6113\n",
            "Epoch: 27/50... Step: 3300... Loss: 1.3962... Val Loss: 1.6183\n",
            "Epoch: 27/50... Step: 3310... Loss: 1.4241... Val Loss: 1.6131\n",
            "Epoch: 27/50... Step: 3320... Loss: 1.4417... Val Loss: 1.6127\n",
            "Epoch: 27/50... Step: 3330... Loss: 1.4285... Val Loss: 1.6122\n",
            "Epoch: 27/50... Step: 3340... Loss: 1.4481... Val Loss: 1.6155\n",
            "Epoch: 27/50... Step: 3350... Loss: 1.4147... Val Loss: 1.6050\n",
            "Epoch: 27/50... Step: 3360... Loss: 1.3741... Val Loss: 1.6112\n",
            "Epoch: 27/50... Step: 3370... Loss: 1.3998... Val Loss: 1.6106\n",
            "Epoch: 27/50... Step: 3380... Loss: 1.3964... Val Loss: 1.6137\n",
            "Epoch: 27/50... Step: 3390... Loss: 1.3799... Val Loss: 1.6043\n",
            "Epoch: 27/50... Step: 3400... Loss: 1.4275... Val Loss: 1.6072\n",
            "Epoch: 28/50... Step: 3410... Loss: 1.4285... Val Loss: 1.6110\n",
            "Epoch: 28/50... Step: 3420... Loss: 1.4007... Val Loss: 1.6147\n",
            "Epoch: 28/50... Step: 3430... Loss: 1.4107... Val Loss: 1.6147\n",
            "Epoch: 28/50... Step: 3440... Loss: 1.3762... Val Loss: 1.6119\n",
            "Epoch: 28/50... Step: 3450... Loss: 1.3914... Val Loss: 1.6157\n",
            "Epoch: 28/50... Step: 3460... Loss: 1.4311... Val Loss: 1.6066\n",
            "Epoch: 28/50... Step: 3470... Loss: 1.4299... Val Loss: 1.6012\n",
            "Epoch: 28/50... Step: 3480... Loss: 1.4050... Val Loss: 1.6016\n",
            "Epoch: 28/50... Step: 3490... Loss: 1.3934... Val Loss: 1.6089\n",
            "Epoch: 28/50... Step: 3500... Loss: 1.3993... Val Loss: 1.5990\n",
            "Epoch: 28/50... Step: 3510... Loss: 1.4198... Val Loss: 1.5979\n",
            "Epoch: 28/50... Step: 3520... Loss: 1.4340... Val Loss: 1.6007\n",
            "Epoch: 29/50... Step: 3530... Loss: 1.4138... Val Loss: 1.6001\n",
            "Epoch: 29/50... Step: 3540... Loss: 1.3833... Val Loss: 1.5963\n",
            "Epoch: 29/50... Step: 3550... Loss: 1.3723... Val Loss: 1.6019\n",
            "Epoch: 29/50... Step: 3560... Loss: 1.4087... Val Loss: 1.6070\n",
            "Epoch: 29/50... Step: 3570... Loss: 1.4326... Val Loss: 1.5995\n",
            "Epoch: 29/50... Step: 3580... Loss: 1.4333... Val Loss: 1.5978\n",
            "Epoch: 29/50... Step: 3590... Loss: 1.4089... Val Loss: 1.5982\n",
            "Epoch: 29/50... Step: 3600... Loss: 1.4193... Val Loss: 1.5972\n",
            "Epoch: 29/50... Step: 3610... Loss: 1.3630... Val Loss: 1.5970\n",
            "Epoch: 29/50... Step: 3620... Loss: 1.3928... Val Loss: 1.5932\n",
            "Epoch: 29/50... Step: 3630... Loss: 1.3611... Val Loss: 1.5934\n",
            "Epoch: 29/50... Step: 3640... Loss: 1.4285... Val Loss: 1.5938\n",
            "Epoch: 29/50... Step: 3650... Loss: 1.4004... Val Loss: 1.5958\n",
            "Epoch: 30/50... Step: 3660... Loss: 1.3694... Val Loss: 1.5915\n",
            "Epoch: 30/50... Step: 3670... Loss: 1.3788... Val Loss: 1.5979\n",
            "Epoch: 30/50... Step: 3680... Loss: 1.3872... Val Loss: 1.5994\n",
            "Epoch: 30/50... Step: 3690... Loss: 1.3910... Val Loss: 1.5954\n",
            "Epoch: 30/50... Step: 3700... Loss: 1.4321... Val Loss: 1.5949\n",
            "Epoch: 30/50... Step: 3710... Loss: 1.3529... Val Loss: 1.6030\n",
            "Epoch: 30/50... Step: 3720... Loss: 1.3984... Val Loss: 1.5921\n",
            "Epoch: 30/50... Step: 3730... Loss: 1.3756... Val Loss: 1.5871\n",
            "Epoch: 30/50... Step: 3740... Loss: 1.4028... Val Loss: 1.5918\n",
            "Epoch: 30/50... Step: 3750... Loss: 1.3736... Val Loss: 1.5917\n",
            "Epoch: 30/50... Step: 3760... Loss: 1.3838... Val Loss: 1.5912\n",
            "Epoch: 30/50... Step: 3770... Loss: 1.3729... Val Loss: 1.5894\n",
            "Epoch: 30/50... Step: 3780... Loss: 1.4352... Val Loss: 1.5902\n",
            "Epoch: 31/50... Step: 3790... Loss: 1.3819... Val Loss: 1.5903\n",
            "Epoch: 31/50... Step: 3800... Loss: 1.3897... Val Loss: 1.5923\n",
            "Epoch: 31/50... Step: 3810... Loss: 1.3973... Val Loss: 1.5971\n",
            "Epoch: 31/50... Step: 3820... Loss: 1.4065... Val Loss: 1.5918\n",
            "Epoch: 31/50... Step: 3830... Loss: 1.3836... Val Loss: 1.5954\n",
            "Epoch: 31/50... Step: 3840... Loss: 1.3668... Val Loss: 1.5918\n",
            "Epoch: 31/50... Step: 3850... Loss: 1.3935... Val Loss: 1.5869\n",
            "Epoch: 31/50... Step: 3860... Loss: 1.3783... Val Loss: 1.5902\n",
            "Epoch: 31/50... Step: 3870... Loss: 1.3777... Val Loss: 1.5882\n",
            "Epoch: 31/50... Step: 3880... Loss: 1.3645... Val Loss: 1.5841\n",
            "Epoch: 31/50... Step: 3890... Loss: 1.3697... Val Loss: 1.5816\n",
            "Epoch: 31/50... Step: 3900... Loss: 1.3568... Val Loss: 1.5874\n",
            "Epoch: 32/50... Step: 3910... Loss: 1.4096... Val Loss: 1.5821\n",
            "Epoch: 32/50... Step: 3920... Loss: 1.3671... Val Loss: 1.5858\n",
            "Epoch: 32/50... Step: 3930... Loss: 1.3600... Val Loss: 1.5878\n",
            "Epoch: 32/50... Step: 3940... Loss: 1.3746... Val Loss: 1.5870\n",
            "Epoch: 32/50... Step: 3950... Loss: 1.3951... Val Loss: 1.5826\n",
            "Epoch: 32/50... Step: 3960... Loss: 1.3825... Val Loss: 1.5858\n",
            "Epoch: 32/50... Step: 3970... Loss: 1.3887... Val Loss: 1.5930\n",
            "Epoch: 32/50... Step: 3980... Loss: 1.3703... Val Loss: 1.5824\n",
            "Epoch: 32/50... Step: 3990... Loss: 1.3380... Val Loss: 1.5845\n",
            "Epoch: 32/50... Step: 4000... Loss: 1.3422... Val Loss: 1.5814\n",
            "Epoch: 32/50... Step: 4010... Loss: 1.3609... Val Loss: 1.5824\n",
            "Epoch: 32/50... Step: 4020... Loss: 1.3395... Val Loss: 1.5781\n",
            "Epoch: 32/50... Step: 4030... Loss: 1.3708... Val Loss: 1.5814\n",
            "Epoch: 33/50... Step: 4040... Loss: 1.3764... Val Loss: 1.5835\n",
            "Epoch: 33/50... Step: 4050... Loss: 1.3567... Val Loss: 1.5853\n",
            "Epoch: 33/50... Step: 4060... Loss: 1.3687... Val Loss: 1.5851\n",
            "Epoch: 33/50... Step: 4070... Loss: 1.3329... Val Loss: 1.5799\n",
            "Epoch: 33/50... Step: 4080... Loss: 1.3718... Val Loss: 1.5869\n",
            "Epoch: 33/50... Step: 4090... Loss: 1.3954... Val Loss: 1.5889\n",
            "Epoch: 33/50... Step: 4100... Loss: 1.3874... Val Loss: 1.5788\n",
            "Epoch: 33/50... Step: 4110... Loss: 1.3571... Val Loss: 1.5745\n",
            "Epoch: 33/50... Step: 4120... Loss: 1.3547... Val Loss: 1.5789\n",
            "Epoch: 33/50... Step: 4130... Loss: 1.3604... Val Loss: 1.5792\n",
            "Epoch: 33/50... Step: 4140... Loss: 1.3663... Val Loss: 1.5759\n",
            "Epoch: 33/50... Step: 4150... Loss: 1.3848... Val Loss: 1.5802\n",
            "Epoch: 34/50... Step: 4160... Loss: 1.3771... Val Loss: 1.5770\n",
            "Epoch: 34/50... Step: 4170... Loss: 1.3352... Val Loss: 1.5760\n",
            "Epoch: 34/50... Step: 4180... Loss: 1.3361... Val Loss: 1.5773\n",
            "Epoch: 34/50... Step: 4190... Loss: 1.3597... Val Loss: 1.5815\n",
            "Epoch: 34/50... Step: 4200... Loss: 1.3723... Val Loss: 1.5762\n",
            "Epoch: 34/50... Step: 4210... Loss: 1.3729... Val Loss: 1.5761\n",
            "Epoch: 34/50... Step: 4220... Loss: 1.3695... Val Loss: 1.5754\n",
            "Epoch: 34/50... Step: 4230... Loss: 1.3715... Val Loss: 1.5769\n",
            "Epoch: 34/50... Step: 4240... Loss: 1.3300... Val Loss: 1.5835\n",
            "Epoch: 34/50... Step: 4250... Loss: 1.3491... Val Loss: 1.5786\n",
            "Epoch: 34/50... Step: 4260... Loss: 1.3234... Val Loss: 1.5738\n",
            "Epoch: 34/50... Step: 4270... Loss: 1.3848... Val Loss: 1.5713\n",
            "Epoch: 34/50... Step: 4280... Loss: 1.3566... Val Loss: 1.5757\n",
            "Epoch: 35/50... Step: 4290... Loss: 1.3281... Val Loss: 1.5731\n",
            "Epoch: 35/50... Step: 4300... Loss: 1.3354... Val Loss: 1.5767\n",
            "Epoch: 35/50... Step: 4310... Loss: 1.3493... Val Loss: 1.5790\n",
            "Epoch: 35/50... Step: 4320... Loss: 1.3575... Val Loss: 1.5721\n",
            "Epoch: 35/50... Step: 4330... Loss: 1.3887... Val Loss: 1.5717\n",
            "Epoch: 35/50... Step: 4340... Loss: 1.3124... Val Loss: 1.5790\n",
            "Epoch: 35/50... Step: 4350... Loss: 1.3555... Val Loss: 1.5788\n",
            "Epoch: 35/50... Step: 4360... Loss: 1.3440... Val Loss: 1.5714\n",
            "Epoch: 35/50... Step: 4370... Loss: 1.3527... Val Loss: 1.5739\n",
            "Epoch: 35/50... Step: 4380... Loss: 1.3333... Val Loss: 1.5734\n",
            "Epoch: 35/50... Step: 4390... Loss: 1.3444... Val Loss: 1.5727\n",
            "Epoch: 35/50... Step: 4400... Loss: 1.3416... Val Loss: 1.5737\n",
            "Epoch: 35/50... Step: 4410... Loss: 1.3988... Val Loss: 1.5738\n",
            "Epoch: 36/50... Step: 4420... Loss: 1.3485... Val Loss: 1.5763\n",
            "Epoch: 36/50... Step: 4430... Loss: 1.3468... Val Loss: 1.5729\n",
            "Epoch: 36/50... Step: 4440... Loss: 1.3497... Val Loss: 1.5763\n",
            "Epoch: 36/50... Step: 4450... Loss: 1.3590... Val Loss: 1.5738\n",
            "Epoch: 36/50... Step: 4460... Loss: 1.3363... Val Loss: 1.5748\n",
            "Epoch: 36/50... Step: 4470... Loss: 1.3171... Val Loss: 1.5703\n",
            "Epoch: 36/50... Step: 4480... Loss: 1.3701... Val Loss: 1.5692\n",
            "Epoch: 36/50... Step: 4490... Loss: 1.3417... Val Loss: 1.5676\n",
            "Epoch: 36/50... Step: 4500... Loss: 1.3367... Val Loss: 1.5677\n",
            "Epoch: 36/50... Step: 4510... Loss: 1.3261... Val Loss: 1.5684\n",
            "Epoch: 36/50... Step: 4520... Loss: 1.3344... Val Loss: 1.5652\n",
            "Epoch: 36/50... Step: 4530... Loss: 1.3142... Val Loss: 1.5727\n",
            "Epoch: 37/50... Step: 4540... Loss: 1.3629... Val Loss: 1.5653\n",
            "Epoch: 37/50... Step: 4550... Loss: 1.3192... Val Loss: 1.5658\n",
            "Epoch: 37/50... Step: 4560... Loss: 1.3247... Val Loss: 1.5689\n",
            "Epoch: 37/50... Step: 4570... Loss: 1.3327... Val Loss: 1.5700\n",
            "Epoch: 37/50... Step: 4580... Loss: 1.3565... Val Loss: 1.5659\n",
            "Epoch: 37/50... Step: 4590... Loss: 1.3528... Val Loss: 1.5663\n",
            "Epoch: 37/50... Step: 4600... Loss: 1.3505... Val Loss: 1.5711\n",
            "Epoch: 37/50... Step: 4610... Loss: 1.3395... Val Loss: 1.5654\n",
            "Epoch: 37/50... Step: 4620... Loss: 1.2983... Val Loss: 1.5721\n",
            "Epoch: 37/50... Step: 4630... Loss: 1.3054... Val Loss: 1.5685\n",
            "Epoch: 37/50... Step: 4640... Loss: 1.3192... Val Loss: 1.5685\n",
            "Epoch: 37/50... Step: 4650... Loss: 1.2950... Val Loss: 1.5659\n",
            "Epoch: 37/50... Step: 4660... Loss: 1.3225... Val Loss: 1.5709\n",
            "Epoch: 38/50... Step: 4670... Loss: 1.3474... Val Loss: 1.5719\n",
            "Epoch: 38/50... Step: 4680... Loss: 1.3304... Val Loss: 1.5741\n",
            "Epoch: 38/50... Step: 4690... Loss: 1.3243... Val Loss: 1.5748\n",
            "Epoch: 38/50... Step: 4700... Loss: 1.3062... Val Loss: 1.5693\n",
            "Epoch: 38/50... Step: 4710... Loss: 1.3289... Val Loss: 1.5686\n",
            "Epoch: 38/50... Step: 4720... Loss: 1.3540... Val Loss: 1.5700\n",
            "Epoch: 38/50... Step: 4730... Loss: 1.3550... Val Loss: 1.5651\n",
            "Epoch: 38/50... Step: 4740... Loss: 1.3254... Val Loss: 1.5614\n",
            "Epoch: 38/50... Step: 4750... Loss: 1.3158... Val Loss: 1.5640\n",
            "Epoch: 38/50... Step: 4760... Loss: 1.3256... Val Loss: 1.5650\n",
            "Epoch: 38/50... Step: 4770... Loss: 1.3124... Val Loss: 1.5650\n",
            "Epoch: 38/50... Step: 4780... Loss: 1.3334... Val Loss: 1.5660\n",
            "Epoch: 39/50... Step: 4790... Loss: 1.3508... Val Loss: 1.5647\n",
            "Epoch: 39/50... Step: 4800... Loss: 1.3130... Val Loss: 1.5664\n",
            "Epoch: 39/50... Step: 4810... Loss: 1.3071... Val Loss: 1.5683\n",
            "Epoch: 39/50... Step: 4820... Loss: 1.3325... Val Loss: 1.5679\n",
            "Epoch: 39/50... Step: 4830... Loss: 1.3430... Val Loss: 1.5628\n",
            "Epoch: 39/50... Step: 4840... Loss: 1.3517... Val Loss: 1.5647\n",
            "Epoch: 39/50... Step: 4850... Loss: 1.3252... Val Loss: 1.5609\n",
            "Epoch: 39/50... Step: 4860... Loss: 1.3424... Val Loss: 1.5612\n",
            "Epoch: 39/50... Step: 4870... Loss: 1.2865... Val Loss: 1.5638\n",
            "Epoch: 39/50... Step: 4880... Loss: 1.3151... Val Loss: 1.5626\n",
            "Epoch: 39/50... Step: 4890... Loss: 1.2847... Val Loss: 1.5634\n",
            "Epoch: 39/50... Step: 4900... Loss: 1.3605... Val Loss: 1.5621\n",
            "Epoch: 39/50... Step: 4910... Loss: 1.3082... Val Loss: 1.5655\n",
            "Epoch: 40/50... Step: 4920... Loss: 1.2885... Val Loss: 1.5610\n",
            "Epoch: 40/50... Step: 4930... Loss: 1.3074... Val Loss: 1.5647\n",
            "Epoch: 40/50... Step: 4940... Loss: 1.3162... Val Loss: 1.5678\n",
            "Epoch: 40/50... Step: 4950... Loss: 1.3094... Val Loss: 1.5607\n",
            "Epoch: 40/50... Step: 4960... Loss: 1.3523... Val Loss: 1.5623\n",
            "Epoch: 40/50... Step: 4970... Loss: 1.2732... Val Loss: 1.5670\n",
            "Epoch: 40/50... Step: 4980... Loss: 1.3304... Val Loss: 1.5654\n",
            "Epoch: 40/50... Step: 4990... Loss: 1.3041... Val Loss: 1.5601\n",
            "Epoch: 40/50... Step: 5000... Loss: 1.3191... Val Loss: 1.5635\n",
            "Epoch: 40/50... Step: 5010... Loss: 1.2990... Val Loss: 1.5634\n",
            "Epoch: 40/50... Step: 5020... Loss: 1.3066... Val Loss: 1.5629\n",
            "Epoch: 40/50... Step: 5030... Loss: 1.3026... Val Loss: 1.5640\n",
            "Epoch: 40/50... Step: 5040... Loss: 1.3633... Val Loss: 1.5668\n",
            "Epoch: 41/50... Step: 5050... Loss: 1.3175... Val Loss: 1.5648\n",
            "Epoch: 41/50... Step: 5060... Loss: 1.3160... Val Loss: 1.5634\n",
            "Epoch: 41/50... Step: 5070... Loss: 1.3227... Val Loss: 1.5650\n",
            "Epoch: 41/50... Step: 5080... Loss: 1.3252... Val Loss: 1.5606\n",
            "Epoch: 41/50... Step: 5090... Loss: 1.3019... Val Loss: 1.5673\n",
            "Epoch: 41/50... Step: 5100... Loss: 1.3024... Val Loss: 1.5624\n",
            "Epoch: 41/50... Step: 5110... Loss: 1.3245... Val Loss: 1.5579\n",
            "Epoch: 41/50... Step: 5120... Loss: 1.3146... Val Loss: 1.5577\n",
            "Epoch: 41/50... Step: 5130... Loss: 1.3167... Val Loss: 1.5587\n",
            "Epoch: 41/50... Step: 5140... Loss: 1.3010... Val Loss: 1.5617\n",
            "Epoch: 41/50... Step: 5150... Loss: 1.2984... Val Loss: 1.5570\n",
            "Epoch: 41/50... Step: 5160... Loss: 1.2783... Val Loss: 1.5624\n",
            "Epoch: 42/50... Step: 5170... Loss: 1.3384... Val Loss: 1.5578\n",
            "Epoch: 42/50... Step: 5180... Loss: 1.2999... Val Loss: 1.5598\n",
            "Epoch: 42/50... Step: 5190... Loss: 1.3001... Val Loss: 1.5591\n",
            "Epoch: 42/50... Step: 5200... Loss: 1.3166... Val Loss: 1.5602\n",
            "Epoch: 42/50... Step: 5210... Loss: 1.3328... Val Loss: 1.5562\n",
            "Epoch: 42/50... Step: 5220... Loss: 1.3113... Val Loss: 1.5566\n",
            "Epoch: 42/50... Step: 5230... Loss: 1.3216... Val Loss: 1.5577\n",
            "Epoch: 42/50... Step: 5240... Loss: 1.3024... Val Loss: 1.5552\n",
            "Epoch: 42/50... Step: 5250... Loss: 1.2705... Val Loss: 1.5634\n",
            "Epoch: 42/50... Step: 5260... Loss: 1.2698... Val Loss: 1.5572\n",
            "Epoch: 42/50... Step: 5270... Loss: 1.2966... Val Loss: 1.5592\n",
            "Epoch: 42/50... Step: 5280... Loss: 1.2667... Val Loss: 1.5560\n",
            "Epoch: 42/50... Step: 5290... Loss: 1.2924... Val Loss: 1.5638\n",
            "Epoch: 43/50... Step: 5300... Loss: 1.3110... Val Loss: 1.5589\n",
            "Epoch: 43/50... Step: 5310... Loss: 1.2984... Val Loss: 1.5628\n",
            "Epoch: 43/50... Step: 5320... Loss: 1.2985... Val Loss: 1.5654\n",
            "Epoch: 43/50... Step: 5330... Loss: 1.2770... Val Loss: 1.5574\n",
            "Epoch: 43/50... Step: 5340... Loss: 1.2941... Val Loss: 1.5592\n",
            "Epoch: 43/50... Step: 5350... Loss: 1.3246... Val Loss: 1.5599\n",
            "Epoch: 43/50... Step: 5360... Loss: 1.3147... Val Loss: 1.5549\n",
            "Epoch: 43/50... Step: 5370... Loss: 1.3007... Val Loss: 1.5521\n",
            "Epoch: 43/50... Step: 5380... Loss: 1.2809... Val Loss: 1.5552\n",
            "Epoch: 43/50... Step: 5390... Loss: 1.2911... Val Loss: 1.5538\n",
            "Epoch: 43/50... Step: 5400... Loss: 1.2927... Val Loss: 1.5521\n",
            "Epoch: 43/50... Step: 5410... Loss: 1.3150... Val Loss: 1.5601\n",
            "Epoch: 44/50... Step: 5420... Loss: 1.3198... Val Loss: 1.5582\n",
            "Epoch: 44/50... Step: 5430... Loss: 1.2906... Val Loss: 1.5581\n",
            "Epoch: 44/50... Step: 5440... Loss: 1.2756... Val Loss: 1.5572\n",
            "Epoch: 44/50... Step: 5450... Loss: 1.3068... Val Loss: 1.5567\n",
            "Epoch: 44/50... Step: 5460... Loss: 1.3112... Val Loss: 1.5536\n",
            "Epoch: 44/50... Step: 5470... Loss: 1.3036... Val Loss: 1.5560\n",
            "Epoch: 44/50... Step: 5480... Loss: 1.3131... Val Loss: 1.5546\n",
            "Epoch: 44/50... Step: 5490... Loss: 1.3151... Val Loss: 1.5531\n",
            "Epoch: 44/50... Step: 5500... Loss: 1.2644... Val Loss: 1.5561\n",
            "Epoch: 44/50... Step: 5510... Loss: 1.2985... Val Loss: 1.5538\n",
            "Epoch: 44/50... Step: 5520... Loss: 1.2445... Val Loss: 1.5544\n",
            "Epoch: 44/50... Step: 5530... Loss: 1.3367... Val Loss: 1.5521\n",
            "Epoch: 44/50... Step: 5540... Loss: 1.2789... Val Loss: 1.5587\n",
            "Epoch: 45/50... Step: 5550... Loss: 1.2584... Val Loss: 1.5525\n",
            "Epoch: 45/50... Step: 5560... Loss: 1.2708... Val Loss: 1.5554\n",
            "Epoch: 45/50... Step: 5570... Loss: 1.2918... Val Loss: 1.5567\n",
            "Epoch: 45/50... Step: 5580... Loss: 1.2813... Val Loss: 1.5554\n",
            "Epoch: 45/50... Step: 5590... Loss: 1.3294... Val Loss: 1.5524\n",
            "Epoch: 45/50... Step: 5600... Loss: 1.2512... Val Loss: 1.5588\n",
            "Epoch: 45/50... Step: 5610... Loss: 1.3100... Val Loss: 1.5575\n",
            "Epoch: 45/50... Step: 5620... Loss: 1.2829... Val Loss: 1.5570\n",
            "Epoch: 45/50... Step: 5630... Loss: 1.2850... Val Loss: 1.5586\n",
            "Epoch: 45/50... Step: 5640... Loss: 1.2792... Val Loss: 1.5553\n",
            "Epoch: 45/50... Step: 5650... Loss: 1.2833... Val Loss: 1.5551\n",
            "Epoch: 45/50... Step: 5660... Loss: 1.2785... Val Loss: 1.5566\n",
            "Epoch: 45/50... Step: 5670... Loss: 1.3447... Val Loss: 1.5630\n",
            "Epoch: 46/50... Step: 5680... Loss: 1.2984... Val Loss: 1.5576\n",
            "Epoch: 46/50... Step: 5690... Loss: 1.2913... Val Loss: 1.5551\n",
            "Epoch: 46/50... Step: 5700... Loss: 1.2957... Val Loss: 1.5582\n",
            "Epoch: 46/50... Step: 5710... Loss: 1.3059... Val Loss: 1.5523\n",
            "Epoch: 46/50... Step: 5720... Loss: 1.2806... Val Loss: 1.5543\n",
            "Epoch: 46/50... Step: 5730... Loss: 1.2730... Val Loss: 1.5523\n",
            "Epoch: 46/50... Step: 5740... Loss: 1.3140... Val Loss: 1.5510\n",
            "Epoch: 46/50... Step: 5750... Loss: 1.2964... Val Loss: 1.5509\n",
            "Epoch: 46/50... Step: 5760... Loss: 1.2831... Val Loss: 1.5492\n",
            "Epoch: 46/50... Step: 5770... Loss: 1.2806... Val Loss: 1.5491\n",
            "Epoch: 46/50... Step: 5780... Loss: 1.2664... Val Loss: 1.5490\n",
            "Epoch: 46/50... Step: 5790... Loss: 1.2543... Val Loss: 1.5564\n",
            "Epoch: 47/50... Step: 5800... Loss: 1.3129... Val Loss: 1.5505\n",
            "Epoch: 47/50... Step: 5810... Loss: 1.2812... Val Loss: 1.5509\n",
            "Epoch: 47/50... Step: 5820... Loss: 1.2865... Val Loss: 1.5514\n",
            "Epoch: 47/50... Step: 5830... Loss: 1.2877... Val Loss: 1.5544\n",
            "Epoch: 47/50... Step: 5840... Loss: 1.2950... Val Loss: 1.5492\n",
            "Epoch: 47/50... Step: 5850... Loss: 1.2854... Val Loss: 1.5501\n",
            "Epoch: 47/50... Step: 5860... Loss: 1.2995... Val Loss: 1.5525\n",
            "Epoch: 47/50... Step: 5870... Loss: 1.2811... Val Loss: 1.5496\n",
            "Epoch: 47/50... Step: 5880... Loss: 1.2421... Val Loss: 1.5568\n",
            "Epoch: 47/50... Step: 5890... Loss: 1.2476... Val Loss: 1.5525\n",
            "Epoch: 47/50... Step: 5900... Loss: 1.2755... Val Loss: 1.5538\n",
            "Epoch: 47/50... Step: 5910... Loss: 1.2354... Val Loss: 1.5515\n",
            "Epoch: 47/50... Step: 5920... Loss: 1.2632... Val Loss: 1.5596\n",
            "Epoch: 48/50... Step: 5930... Loss: 1.2912... Val Loss: 1.5519\n",
            "Epoch: 48/50... Step: 5940... Loss: 1.2788... Val Loss: 1.5611\n",
            "Epoch: 48/50... Step: 5950... Loss: 1.2678... Val Loss: 1.5560\n",
            "Epoch: 48/50... Step: 5960... Loss: 1.2509... Val Loss: 1.5514\n",
            "Epoch: 48/50... Step: 5970... Loss: 1.2758... Val Loss: 1.5510\n",
            "Epoch: 48/50... Step: 5980... Loss: 1.2889... Val Loss: 1.5528\n",
            "Epoch: 48/50... Step: 5990... Loss: 1.2936... Val Loss: 1.5516\n",
            "Epoch: 48/50... Step: 6000... Loss: 1.2777... Val Loss: 1.5494\n",
            "Epoch: 48/50... Step: 6010... Loss: 1.2657... Val Loss: 1.5524\n",
            "Epoch: 48/50... Step: 6020... Loss: 1.2689... Val Loss: 1.5525\n",
            "Epoch: 48/50... Step: 6030... Loss: 1.2743... Val Loss: 1.5469\n",
            "Epoch: 48/50... Step: 6040... Loss: 1.2927... Val Loss: 1.5554\n",
            "Epoch: 49/50... Step: 6050... Loss: 1.2876... Val Loss: 1.5565\n",
            "Epoch: 49/50... Step: 6060... Loss: 1.2598... Val Loss: 1.5532\n",
            "Epoch: 49/50... Step: 6070... Loss: 1.2602... Val Loss: 1.5507\n",
            "Epoch: 49/50... Step: 6080... Loss: 1.2762... Val Loss: 1.5503\n",
            "Epoch: 49/50... Step: 6090... Loss: 1.2877... Val Loss: 1.5491\n",
            "Epoch: 49/50... Step: 6100... Loss: 1.2855... Val Loss: 1.5498\n",
            "Epoch: 49/50... Step: 6110... Loss: 1.2938... Val Loss: 1.5498\n",
            "Epoch: 49/50... Step: 6120... Loss: 1.2934... Val Loss: 1.5442\n",
            "Epoch: 49/50... Step: 6130... Loss: 1.2390... Val Loss: 1.5504\n",
            "Epoch: 49/50... Step: 6140... Loss: 1.2713... Val Loss: 1.5484\n",
            "Epoch: 49/50... Step: 6150... Loss: 1.2289... Val Loss: 1.5487\n",
            "Epoch: 49/50... Step: 6160... Loss: 1.3048... Val Loss: 1.5520\n",
            "Epoch: 49/50... Step: 6170... Loss: 1.2496... Val Loss: 1.5583\n",
            "Epoch: 50/50... Step: 6180... Loss: 1.2469... Val Loss: 1.5507\n",
            "Epoch: 50/50... Step: 6190... Loss: 1.2424... Val Loss: 1.5513\n",
            "Epoch: 50/50... Step: 6200... Loss: 1.2648... Val Loss: 1.5533\n",
            "Epoch: 50/50... Step: 6210... Loss: 1.2664... Val Loss: 1.5519\n",
            "Epoch: 50/50... Step: 6220... Loss: 1.3041... Val Loss: 1.5488\n",
            "Epoch: 50/50... Step: 6230... Loss: 1.2247... Val Loss: 1.5530\n",
            "Epoch: 50/50... Step: 6240... Loss: 1.2846... Val Loss: 1.5507\n",
            "Epoch: 50/50... Step: 6250... Loss: 1.2697... Val Loss: 1.5540\n",
            "Epoch: 50/50... Step: 6260... Loss: 1.2694... Val Loss: 1.5546\n",
            "Epoch: 50/50... Step: 6270... Loss: 1.2584... Val Loss: 1.5523\n",
            "Epoch: 50/50... Step: 6280... Loss: 1.2609... Val Loss: 1.5541\n",
            "Epoch: 50/50... Step: 6290... Loss: 1.2521... Val Loss: 1.5522\n",
            "Epoch: 50/50... Step: 6300... Loss: 1.3074... Val Loss: 1.5578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXl-HMaTEFC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_x_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvfewtNwHnyo",
        "colab_type": "text"
      },
      "source": [
        "## **Making Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqytLH8PHrtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFUgxpfsKRTq",
        "colab_type": "text"
      },
      "source": [
        "### Priming and generating text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yXbHc24KJ8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='Line', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWnkC6boKVTt",
        "colab_type": "code",
        "outputId": "8bacd293-8cbe-476b-db25-16d528ea3b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "print(sample(net, 1000, prime='Line', top_k=5))"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Line at all the door\n",
            "I'd say it is that you can bral your love \n",
            "You know you know that I've looked to live tight\n",
            "The same, way I can see you with you \n",
            "When I long to spend your love is the reatoors\n",
            "And I can stop a turning far\n",
            "\n",
            "When you know, that you do\n",
            "\n",
            "Well in my breath to be\n",
            "\n",
            "I keep trying it all I can stand to take a man tight\n",
            "There's no man thit is I need\n",
            "\n",
            "I wanna stop the world to you\n",
            "\n",
            "Take this way I'm gonna do.\n",
            "I'll never let you stop\n",
            "\n",
            "There's a leather a little thing\n",
            "\n",
            "You're my life, before the right\n",
            "It came always tonight, they can leave to stand to be\n",
            "Your love is the world)\n",
            "An that's my looking for my best arally\n",
            "I'd get another love so long \n",
            "I've got your love to my life to me \n",
            "I know is wait on you\n",
            "And will the much a life\n",
            "\n",
            "Well all the loving though \n",
            "And you're love away\n",
            "\n",
            "I wanna get it in love, I know\n",
            "I wanna leave me\n",
            "\n",
            "All I love you\n",
            "\n",
            "There's no minigh of you shaking around through\n",
            "\n",
            "Your soul, then I'm a moneite is to me\n",
            "I lost to be the mirds where I will be what wonder\n",
            "\n",
            "I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiC9D68eKmzi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c9e9580-bf82-43ce-bd74-bd1accd42faa"
      },
      "source": [
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
        "with open('rnn_x_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2y89TBIKnQQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa48f471-3e49-4a7f-998c-05c5b8cb751a"
      },
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=5, prime=\"The sky above\"))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sky above \n",
            "Into my late at your heart\n",
            "And I was, when, I don't care you won't give me\n",
            "\n",
            "When you call me\n",
            "It's not no makes love\n",
            "I was livin' is together.\n",
            "\n",
            "You know you know you wanna know\n",
            "\n",
            "I wanna sex you the semmly soul\n",
            "We say if you wanna be all of my love\n",
            "\n",
            "Though you want it too\n",
            "I wanna know I do to mine\n",
            "And it's the ong and my love that you say \n",
            "And I can stay takes you will not see\n",
            "The way we was started it all\n",
            "I'll go back it all of me\n",
            "I'm gonna show you to show me the same\n",
            "When we want the late in me on, I wanna stop the moment I leave you fall in love words\n",
            "When what I dripping and I can see the right way\n",
            "\n",
            "If I was world will stop in your hands\n",
            "\n",
            "Tell me all my love, my life\n",
            "This life time well you, love me all that me\n",
            "\n",
            "I would stay that who I can break it mine\n",
            "\n",
            "And I was love with everything wanna lay me \n",
            "And the way I'll share the only thing with you\n",
            "\n",
            "You're the one I can't dravees you to me \n",
            "\n",
            "I can't lose it)\n",
            "I'll stand by you \n",
            "When I see you and you're the life and when I saw you baby I don't stop, the one\n",
            "And the only only one time \n",
            "I don't know, that my life, you can live that I would\n",
            "\n",
            "I wanna be the way I've given the story\n",
            "It was what I know I wanna live the world over\n",
            "As long as you gotta be\n",
            "I know you see I can do\n",
            "Take a little show\n",
            "There's nothing you belong\n",
            "I wanna keep a little time walking on \n",
            "You're the one there that I love\n",
            "\n",
            "I know you know, I want you\n",
            "The way the man will shake you she's not a movit,\n",
            "I wanna be you world on the sun and the morning where you are)\n",
            "All that you give me\n",
            "I'd still say, I'd see the same together\n",
            "\n",
            "I want to save to me \n",
            "I wanna know what I can't say\n",
            "And I'd see\n",
            "I want to be with me\n",
            "\n",
            "I know that I'm gonna try in the reatoor I'm all from of your man\n",
            "And you’re sexust my best on to you\n",
            "You know I've let you say I can say \n",
            "Tonight I live and I don't care\n",
            "A wonding a love is for me\n",
            "\n",
            "I'm taken makes and leveng to me anything. \n",
            "Tick me that I was saying\n",
            "Won't it without the stures, you know\n",
            "You cealling the sude and I'm all the way, I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72asTe0vhLlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}